---
title: "PSI_Capstone Pair Project"
author: "Ruei Li Jhang & Chia Hua Lin"
date: "2024-03-25"
output: 
  html_document:
    toc: yes
    toc_depth: 5
    number_sections: yes
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```


```{r message=FALSE, warning=FALSE}

# Loading libraries

library(dplyr)
library(ggplot2)
library(ltm)
library(randomForest)
library(rpart)
library(e1071)
library(mlbench)
library(caTools)
library(caret)
library(pROC)
library(PRROC)
library(ROCR)
library(xgboost)

```



# Data Understanding

The data set was downloaded from Kaggle.com. According to the data description, the data focuses on malicious URL detection. This data helps in the development of cyber security systems that can detect any malicious attempt to gain access and send a sigal to systems to perform the relevant in return.

Data set link: https://www.kaggle.com/datasets/pilarpieiro/tabular-dataset-ready-for-malicious-url-detection/data

The data set includes URLs and 60 other calculated features. In our analysis, we will not use all the 60 features, but we will select the most important features. In the data description, a list of 6 important features has been provided and they are as follows:

* Basic URL Components

* Domain Information

* Content Analysis

* Host Reputation

* Network Features

* Behavioural Features



```{r}

# # Loading the train data
# train_data <- read.csv('train_dataset.csv')
# train_data <- na.omit(train_data) # Remove rows with any NA/null values
# dim(train_data)
 
# # Loading test data
# test_data <- read.csv('test_dataset.csv')
# test_data <- na.omit(test_data) # Remove rows with any NA/null values
# dim(test_data)

```

The original train data has 6,728,848 observations and the test data contains 1,682,213.


**Due to the computational resources, we will randomly select 200,000 observations for training and 100,000 observations for testing.** 



```{r}

# # Set seed for reproducibility
# set.seed(2024)

# # Define the number of samples you want for each label category
# num_samples_per_label <- 1000000

# # Sample from each label category
# train_data_sample <- train_data %>%
#    group_by(label) %>%
#    sample_n(num_samples_per_label, replace = TRUE)

# # Test data
# test_data_sample <- test_data[sample(nrow(test_data), 1000000), ]

# # Write train_data2 to CSV
# write.csv(train_data_sample, file = "train_data_sample.csv", row.names = FALSE)
# write.csv(test_data_sample, file = "test_data_sample.csv", row.names = FALSE)

```




## Loading the Save Data

```{r}

# Loading the train data
train_data <- read.csv('train_data_sample.csv')
train_data <- na.omit(train_data) # Remove rows with any NA/null values
dim(train_data)

# Loading the test data
test_data <- read.csv('test_data_sample.csv')
test_data <- na.omit(test_data) # Remove rows with any NA/null values
dim(test_data)

```




## Selecting Necessary Columns

We selected specific columns from the train_data data frame. The columns selected include various features related to URLs as:

* Whether the URL contains certain elements like login, client, server, admin, IP,
* Whether it is shortened,
* Its length, entropy,
* Its counts of various characters and components,
* Features related to the length
* Components of the path,
* Query, sub domain, and primary domain of the URL.
* Label column indicating some classification or labeling information associated with each URL.


```{r}

# Train data
train_data <- train_data %>%
  dplyr::select(url, source, url_has_login, url_has_client, url_has_server,
                url_has_admin, url_has_ip, url_isshorted, url_len, 
                url_entropy, url_count_dot, url_count_https, url_count_http,
                url_count_perc, url_count_hyphen, url_count_www,
                url_count_hash, url_count_semicolon, url_count_underscore, 
                url_count_ques, url_count_equal, url_count_amp,
                url_count_letter, url_count_digit,
                url_count_sensitive_financial_words, 
                url_count_sensitive_words, path_len, query_len, 
                query_count_components, pdomain_len, subdomain_len, 
                subdomain_count_dot, label
                )

```




## Exploratory Data Analysis

During EDA, we are going to perform tasks such as:

* Summarizing the distribution of each variable (count and distribution)
* Visualizing relationships between variables (scatter plots, box plots)
* Detecting outliers or anomalies (histograms)
* Exploring correlations between variables
* Identifying potential patterns or trends in the data


### Binary Variables

Here, we're preparing for Exploratory Data Analysis (EDA) by selecting specific columns from the train_data dataframe. We are identifying binary variables within the train_dataset. The criteria involve checking if the column is numeric and if it possesses only two unique values. If these conditions are satisfied, indicating that the column indeed contains binary data, we add it to the binary_vars list.


```{r}

# Initialize an empty list to store binary variables
binary_vars <- list()

# Loop through each column in the dataset
for (col in names(train_data)) {
  
  # Check if the column is numeric and has only two unique values
  if (is.numeric(train_data[[col]]) && length(unique(train_data[[col]])) == 2) {
    
    # If the condition is met, add the column to the list
    binary_vars[[col]] <- train_data[[col]]
  }
}

# Convert the list of binary variables to a dataframe
binary_df <- as.data.frame(binary_vars)

# Convert all columns in binary_df to factors
binary_df <- as.data.frame(lapply(binary_df, as.factor))

# Overview
head(binary_df, 5)

```




#### Distribution of url_has_login

```{r}

# Plotting the count of url_has_login
ggplot(binary_df, aes(x = url_has_login, fill = url_has_login)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of url_has_login",
       x = "url_has_login",
       y = "Count") +
  theme_minimal() +
  facet_wrap(~ label, labeller = labeller(label = c("0" = "benign", "1" = "Malicious")))

```

For the URLs that are no malicious, 99986 had no log in while 14 had log in. For the malicious URLs, 92849 had no log in while 7151 had log in.




#### Distribution of url_has_client

```{r}

ggplot(binary_df, aes(x = url_has_client, fill = url_has_client)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of url_has_client",
       x = "url_has_client",
       y = "Count") +
  theme_minimal() +
  facet_wrap(~ label, labeller = labeller(label = c("0" = "benign", "1" = "Malicious")))

```

For the URLs that are no malicious, 99987 had no client in while 13 had client. For the malicious URLs, 99123 had no client in while 877 had client.




#### Distribution of url_has_server

```{r}

# Plotting the count of url_has_server
ggplot(binary_df, aes(x = url_has_server, fill = url_has_server)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of url_has_server",
       x = "url_has_server",
       y = "Count" ) +
  theme_minimal() +
  facet_wrap(~ label, labeller = labeller(label = c ("0" = "benign", "1" = "Malicious")))

```

For the UELs that are not malicious, 99839 had no server in while 161 had server. For the malicious URLs, 99512 had no server in while 488 had server.




#### Distribution of url_has_admin

```{r}

#plotting the count of url_has_admin
ggplot(binary_df, aes(x = url_has_admin, fill = url_has_admin)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of url_has_admin",
       x = "url_has_admin",
       y = "Count" ) +
  theme_minimal() +
  facet_wrap(~ label, labeller = labeller(label = c("0" = "benign", "1" = "Milicious")))

```

For the URLs that are not malicious, 99950 had no admin in while 50 had admin. For the malicious URLs, 98442 had no admin while 1558 had admin.




#### Distribution of url_has_ip

```{r}

# Plotting the count of url_has_ip
ggplot(binary_df, aes(x = url_has_ip, fill = url_has_ip)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)), 
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of url_has_ip",
       x = "url_has_ip",
       y = "Count" ) +
  theme_minimal() +
  facet_wrap(~ label, labeller = labeller(label = c("0" = "benign", "1" = "Malicious")))

```

For the URLs that are not malicious, all had no IP. For the malicious URLs, 98374 had no IP in while 1626 had IP.




#### Distribution of url_isshorted

```{r}

# Plotting the count of url_isshorted
ggplot(binary_df, aes(x = url_isshorted, fill = url_isshorted)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of url_isshorted",
       x = "url_isshorted",
       y = "Count" ) +
  theme_minimal() +
  facet_wrap(~ label, labeller = labeller(label = c("0" = "benign", "1" = "Milicious")))

```

For the URLs that are no milicious, 4475 were shortened in while 95525 were not. For the malicious URLs, 7091 were shortened in while 92909 were not.





#### Distribution of Label

```{r}

# Plotting the count of label
ggplot(binary_df, aes(x = label, fill = label)) +
  geom_bar(stat = "count", position = "dodge") +
  geom_text(stat = "count", aes(label = after_stat(count)),
            vjust = -0.2, position = position_dodge(width = 0.8)) +
  labs(title = "Distribution of label",
       x = "label",
       y = "Count" ) +
  theme_minimal()

```

Out target variable which is Label, is was balanced.




### Distribution of Numerical Variables

```{r}

# Selecting all columns except the ones mentioned in binary
numerical_columns <- train_data[, !names(train_data) %in% names(binary_df)]

# Remove the first two columns
numerical_columns <- numerical_columns %>%
  dplyr::select(-url, -source)

```




#### Distribution of URL_LEN

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_len)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Length", x = "URL Length", y = "Frequency")

```

The URL len is positively skewed meaning the majority of the URLs have a shorter length. However, we can already see the presence of outliers in the URL length column. We will treat this by dropping all the outliers.




#### Distribution of url_entropy

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_entropy)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Entropy", x = "URL Entropy", y = "Frequency")

```

URL entropy is almost normally distributed with a slight negative skew.




#### Distribution of url_count_dot

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_dot)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Dot Count", x = "Dot Count", y = "Frequency")

```




#### Distribution of url_count_https

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_https)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL HTTPS Count", x = "HTTPS Count", y = "Frequency")

```




#### Distribution of url_count_http

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_http)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL HTTP Count", x = "HTTP Count", y = "Frequency")

```




#### Distribution of url_count_perc

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_perc)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Perc Count", x = "Perc Count", y = "Frequency")

```




#### Distribution of url_count_hyphen

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_hyphen)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Hyphen Count", x = "Hyphen Count", y = "Frequency")

```




#### Distribution of url_count_www

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_www)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL WWW Count", x = "WWW Count", y = "Frequency")

```




#### Distribution of url_count_hash

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_hash)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Hash Count", x = "Hash Count", y = "Frequency")

```




#### Distribution of url_count_semicolon

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_semicolon)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Semicolon Count", x = "Semicolon Count", y = "Frequency")

```




#### Distribution of url_count_underscore

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_underscore)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Underscore Count", x = "Underscore Count", y = "Frequency")

```




#### Distribution of url_count_ques

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_ques)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Question Mark Count", x = "Question Mark Count", y = "Frequency")

```




#### Distribution of url_count_equal

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_equal)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Equal Sign Count", x = "Equal Sign Count", y = "Frequency")

```




#### Distribution of url_count_amp

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_amp)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Ampersands Count", x = "Ampersands Count", y = "Frequency")

```




#### Distribution of url_count_letter

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_letter)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Letter Count", x = "Letter Count", y = "Frequency")

```




#### Distribution of url_count_digit

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_digit)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Digits Count", x = "Digits Count", y = "Frequency")

```




#### Distribution of url_count_sensitive_financial_words

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_sensitive_financial_words)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Sensitive Financial Words Count", 
       x = "Sensitive Financial Words Count", y = "Frequency")

```




#### Distribution of url_count_sensitive_words

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = url_count_sensitive_words)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of URL Sensitive Words Count", 
       x = "Sensitive Words Count", y = "Frequency")

```




#### Distribution of path_len

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = path_len)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Path Length", x = "Path Length", y = "Frequency")

```




#### Distribution of query_len

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = query_len)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Query Length", x = "Query Length", y = "Frequency")

```




#### Distribution of query_count_components

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = query_count_components)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Count of Query Components",
       x = "Count of Query Components", y = "Frequency")

```




#### Distribution of primary domain length

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = pdomain_len)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Length of Primary Domain",
       x = "Domain Length", y = "Frequency")

```




#### Distribution of subdomain len

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = subdomain_len)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Subdomain Length",
       x = "Subdomain Length", y = "Frequency")

```




#### Distribution of subdomain_count_dot

```{r}

# Plotting histogram using ggplot2
ggplot(numerical_columns, aes(x = subdomain_count_dot)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Count of Dot in the Subdomain",
       x = "Count of Subdomain Dot", y = "Frequency")

```

Other than URL entropy, all the variables are positively skewed with a clear sign of large outliers. These outliers could skew our analysis results and therefore, we will deal with them in the Data Preparation section.




### Treating Outliers

We defined a function for identifying and managing outliers within a dataset. The function takes three parameters: the dataset data, a vector specifying the columns to be processed columns, and a threshold determining the outlier deterction criteria, set to 3 standard deviations from the mean. Detected outliers are replaced with NA values, and a summary is printed for each column, indicating the number of outliers removed. Then all the missing valuas (NA) are omitted.

```{r}

# Removing outliers
remove_outliers <- function(data, columns, threshold = 3) {
  # Iterate over each specified column
  for (col in columns) {
    # Calculate mean and standard deviation
    mean_value <- mean(data[[col]], na.rm = TRUE)
    sd_value <- sd(data[[col]], na.rm = TRUE)
    
    # Identify outliers
    outliers <- data[data[[col]] > mean_value + threshold * sd_value |
                     data[[col]] < mean_value - threshold * sd_value, ]
    
    # Treat outliers (replace with NA)
    data[[col]][data[[col]] > mean_value + threshold * sd_value |
                data[[col]] < mean_value - threshold * sd_value ] <- NA
    
    # Print summary
    cat("Outliers removed for column:", col, "\n")
    cat("Number of outliers removed:", nrow(outliers), "\n\n")
  }
  
  return(data)
  
}

# Call the function
train_data_cleaned <- remove_outliers(train_data, names(numerical_columns), threshold = 3)

```

```{r}

# Remove nulls
train_data_cleaned <- na.omit(train_data_cleaned)

```




### Relationship Between Our Target Variable, Label, and All Other Variables


#### Correlation Between Numerical Variables and Target Variable, Label

Then, we perform correlation analysis between numerical variables and a binary label in a dataset. We define a function named correlation_plot that calculates point-biserial correlation coefficients for each numerical variable with respect to the binary label. This function iterates over the numerical columns, computes the correlation coefficient using the biserial.cor function, and appends the results to the global dataframe correlation_data. Finally, the fuction generates a bar plot of correlationts ordered from highest to lowest, providing insights into the strength and direction of association between each numerical variable and the binary label.



```{r}

# Select binary variables
binary_cols <- names(binary_df)

# Remove the label
binary_cols <- setdiff(binary_cols, "label")

# Initialize an empty dataframe to store correlation coefficients
correlation_data <- data.frame(variable = character(),
                               correlation = numeric(),
                               stringsAsFactors = FALSE)

correlation_plot <- function(data, label_column) {
  
  # Select numerical columns
  data2 <- select_if(data, is.numeric)
  
  # Remove the binary variables
  numerical_cols <- data2[, !(names(data2) %in% binary_cols)]
  
  # Iterate over numerical columns
  for (col in names(numerical_cols)) {
    
    # Calculate point-biserial correlation
    corr <- biserial.cor(data[[col]], as.numeric(data[[label_column]]),
                         use = c("all.obs"), level = 2)
    
    # Store correlation coefficient in the global dataframe
    correlation_data <<- bind_rows(correlation_data,
                                   data.frame(variable = col,
                                              correlation = corr))
  }
  
  # Plot correlation coefficients with bars ordered from max to min correlation
  ggplot(na.omit(correlation_data), aes(x = reorder(variable, -correlation), y = correlation)) +
    geom_bar(stat = "identity", fill = "skyblue", color ="black") +
    coord_flip() +
    labs(title = "Point-Biserial Correlation of Numerical Variables with Label",
         x = "Variable",
         y = "Correlation Coefficient") +
    theme_minimal()
}

# Calling the function
correlation_plot(train_data_cleaned, "label")

```




```{r}

# Showing the dataframe
print(correlation_data)

```


The plot above visualizes the data in the data frame printed above. We can see that we have numerical columns that have medium strength correlation with our target variable which is label. Some of the columns had NaNs so we will not be moving forward with any column that had no correlation with our target variable.

It is important to note that all our variables are positively correlated to our target variable

URL entropy has the strongest correlation with our target variable label, followed by URL Length, URL letter count and sub domain length in that order.



#### Correlation Between Binary Varialbe and Target Varialbe Label

In here, we computed the Phi coefficient, a measure of association between two binary variables, specifically between binary variables and a binary label within a dataset. The compute_phi_coefficients function iterates over each column in the dataset, checking if the column is binary(i.e., a factor with two levels). If the condition is met, the Phi coefficient is calculated between the binary variable and the binary label.The results are appended to the result dataframe.Finally, the function generates a bar plot visualizing the Phi coefficients for each binary variable with respect to the label. The function returns both the result dataframe and the genarated plot.

```{r}

# Initialize empty dataframe
result <- data.frame(variable = character(),
                     label = character(),
                     phi_coefficient = numeric(),
                     stringsAsFactors = FALSE)

# Compute phi coefficient
compute_phi_coefficients <- function(data) {
  for (col in names(data)) {
    if (is.binary <- is.factor(data[[col]]) && length(levels(data[[col]])) == 2) {
      phi <- cor(as.numeric(data[[col]]), as.numeric(data$label))
      result <- rbind(result, data.frame(variable = col,
                                         label = "label",
                                         phi_coefficient = phi,
                                         stringsAsFactors = FALSE))
    }
  }
  
  # Plot correlation coefficients 
  plot <- ggplot(na.omit(result), aes(x = reorder(variable, -phi_coefficient), y = phi_coefficient)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "black") +
    coord_flip() +
    labs(title = "Phi Coefficint of Binary Variables with Label",
         x = "Variable",
         y = "Phi Coefficient") +
    theme_minimal()
  
  return(list(result = result, plot = plot))  # Return both result dataframe and plot
}

# Call the function
output <- compute_phi_coefficients(binary_df)

# Plot the bar chart separately
print(output$plot)

```


```{r}

# result dataframe
result_df <- output$result
result_df

```

We can also see the phi coefficient between all the binary variables and our target variable, label. Phi coefficients is a measure of association between two dichotomous variables.It measures how much two binary variables are associated where 1 is perfect association while 0 means no association at all.

The URL with log in has the strongest association with the target variable, followed by Url with IP, then URL with admin in that order. Since all of them had at least a weak association, we are going to use all the 6 binary variables in our next.




# Data Preparation

We will then perform preparation like cleaning, and transforming the data to make it suitable for analysis and modeling. This may iclude handling missing values, encoding categorical variables, scaling features, and splitting the data into training and testing sets.



## Cleaning

In here, we performed data preprocessing steps to prepare the training and test datasets for model training and evaluation. Firstly, we select columns without null values from the correlation_data dataframe and stroe the names of these columns in the num_cols vector. Then, we identify necessary columns for modeling, including both binary columns(binary_cols)and numerical columns(num_cols). Next, we set the seed for reproducibility using set.seed(2004) and downsapmle the train_data_cleaned dataset to 50,000 samples per label group to balace the dataset. We then selected only the necessary columns(necessary_cols) from both the training(train_data_cleaned) and test(test_data) datasets, ensuring consistency in the variables used for modeling between the training and test datasets.


```{r}

# Select the columns without nulls
num_cols <- as.character(na.omit(correlation_data)$variable)

# Necessary columns
necessary_cols <- c(binary_cols, num_cols)

set.seed(2024)
train_data_cleaned <- train_data_cleaned %>% group_by(label) %>% sample_n(50000, replace = TRUE)

# Select only the necessary columns from train_data_clenaned
train <- train_data_cleaned[necessary_cols]

## Cleaning Test data
# Select only the necessary columns from train_data_cleaned
test <- test_data[necessary_cols]

```




## Transforming to Right Data Types

For the training data, we first converted binary columns to factors. We converted the target variable label to a factor, with "0" encoded as "Benign" and "1" as "Malicious". We then shuffle the rows of the training data for randomness using sample and set the seed for reproducibility. Finally, we reset the row indices using row.names(). We did the same for testing set.


```{r}

### Train Data ###
# Convert columns to factors
train[binary_cols] <- lapply(train[binary_cols], factor)

# Convert our target variable to factor
train <- train %>%
  mutate(label = as.factor(ifelse(label == '0', "Benign", "Malicious")))

# Shuffle the rows of train_data
set.seed(2024)
train <- train[sample(nrow(train)), ]

# Reset index using row.names()
row.names(train) <- NULL


### Test data ###
test[binary_cols] <- lapply(test[binary_cols], factor)

# Convert our target variable to factor
test <- test %>%
  mutate(label = as.factor(ifelse(label == '0', "Benign", "Malicious")))

# Shuffle the rows of train_data
set.seed(2024)
test <- test[sample(nrow(test)), ]

# Reset index using row.names()
row.names(test) <- NULL

```




# Modeling

The data set was already split when we downloaded from Kaggle.com, but when selecting, we selected 200,000 observations for training before cleaning and pre-processing, and 100,000 for testing

We sampled a balanced training set.


## Decision Tree

```{r}

# decision tree classifier object
dt_classifier <- rpart(label ~ ., data = train, method = "class")

# Make predictions
dt_predictions <- predict(dt_classifier, newdata = test, type = "class")

# Calculate accuracy
accuracy <- sum(dt_predictions == test$label) / nrow(test)
print(paste("Accuracy:", accuracy))

# Confusion matrix
confusion_matrix <- confusionMatrix(dt_predictions, test$label)

# Print Confusion matrix
print(confusion_matrix)

# AUC using ROC curve
roc_dt <- roc(as.numeric(test$label), as.numeric(dt_predictions))

# AUC for Decision Tree Classifier
auc_dt <- auc(roc_dt)
print(paste("AUC for Decision Tree Classifier:", round(auc_dt, 4)))

```


### ROC Curve For Decision Tree

```{r message=TRUE}

# Binary outcome for the ROC curve
binary_outcome_dt <- test$label

# Create an ROC object for XGBoost
roc_dt <-roc(binary_outcome_dt, as.numeric(dt_predictions))

# Plot ROC curve for XGBoost
plot(roc_dt, col = "blue",
     main = "ROC Curve for XGBoost",
     lwd = 2,
     cex.main = 0.8)

```


The accuracy of the Decision Tree Classifier model is 0.861, indicating that it correctly predicts class of the data points with an accuracy of 80.61%.

We can also see that the model correctly predicted62674 cases as Benign and 17939 cases as Malicious. It mis-classiied 15798 Benign cases as Malicious and 3598 Malicious cases Malicious case as Benign.

The sensitivity of the model is 79.87, indicating the proportion of actual positives(Benign) correctly identified as positive(benign).

The specificity of the model is 83.33, indicating the proportion of actual negatives(Malicious) correctly identified as negative(Malicious).

The Area Under the Curve for the Decision Tree Classifier model is 0.816. This means that the model's ability to distinguish between positive and negative classes stands at 81.6%.




# Logistic Regression

```{r warning=FALSE}

# Fit the Logistic Regression model
set.seed(2024)
log_reg <- glm(label ~ ., data = train, family = "binomial")

# Make Predictions
log_reg_predictions <- predict(log_reg, test)

# Convert predictions to class labels
log_reg_predictions <- as.factor(ifelse(log_reg_predictions > 0.5, 'Malicious', 'Benign'))

# Compute the confusion matrix
confusion_matrix_log_reg <- confusionMatrix(log_reg_predictions, test$label)

# Print the confusion matrix
print(confusion_matrix_log_reg)

# Convert predictions to numeric
log_reg_predictions_numeric <- as.numeric(ifelse(log_reg_predictions == 'Malicious', 1, 0))
binary_label <- ifelse(test$label == 'Malicious', 1, 0)

# Calculate ROC curve
roc_log_reg <- roc(binary_label, log_reg_predictions_numeric)

# AUC for XGBoost
auc_log_reg <- auc(roc_log_reg)

print(paste("AUC for Logistic Regression:", round(auc_log_reg, 4)))

```


### ROC For Logistic Regression


```{r}

# Binary outcome for the ROC curve
binary_outcome_log_reg <- ifelse(test$label == "Alive", 1, 0)

# ROC Curve
roc_log_reg <- roc(log_reg_predictions_numeric, binary_label)

# Plotting ROC Curve for KNN
plot(roc_log_reg, col ="blue",
     main = "ROC Curve for Logistic Regression",
     lwd = 2,
     cex.main = 0.8)

```

The accuracy of the model is 0.8533, which indicates that is correctly predicts the class of the data points with an accuracy of 85.33%.

From the confusion matrix, we can see that the model correctly predicted 69895 Benign cases and 15431 Malicious cases. However, it mis-classified 8577 Benign cases as Malicious and 6097 Malicious cases as Benign.

The sensitivity of the model is 0.8907, which indicates the proportion (89.07%) of actual positives (Benign) correctly identified as positive.

The specificity of the model is 0.7168, which indicates the proportion (71.68%) of actual negatives (Malicious) correctly identified as negative.

The Area Under the Curve is 0.8037, which means, the model's ablility to distinguish between positive and negative classes scored 80.37%.



































